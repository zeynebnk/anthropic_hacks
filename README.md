# anthropic_hacks

## Language Models (can be) Few-Shot Fakers: Disentangling Robust Reasoning from Broad Pattern Matching

We present a series of explorations aiming to draw insights on the relevant yet understudies areas of chain-of-thought faithfulness, robustness and generalizability, and underlying systems of reasoning in language models. 

Our contributions are as follows: we find that language models' chains of thought are not always reliable explanations of model decisions, identifying faked reasoning in language models. Faked and unfaithful reasoning is  often indicative of when they are doing the very oppositeâ€”relying on memorization over extrapolative reasoning. In-distribution examples may present lines of reasoning when reasoning was not used. This has implications on the usefulness of learned reasoning paths in more complex tasks.  
